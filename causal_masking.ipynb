{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fdb614-ab74-4db7-bce5-4a3f4d97e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/tnn/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 7, 512, 1])\n",
      "tensor(0.0002)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from fast_transformers.causal_product import causal_dot_product\n",
    "\n",
    "import laxtnn.utils.causal_product as causal_product\n",
    "\n",
    "\n",
    "n = 512\n",
    "r = 16\n",
    "b = 8\n",
    "h = 7\n",
    "\n",
    "inp = np.random.randn(b,h,n,1)\n",
    "U = np.random.randn(1,h,n,r)\n",
    "V = np.random.randn(1,h,n,r)\n",
    "\n",
    "\n",
    "inp, U, V = [torch.Tensor(x) for x in [inp, U, V]]\n",
    "U = U.expand((b, -1, -1, -1))\n",
    "V = V.expand((b,-1,-1,-1))\n",
    "y = (U @ V.transpose(-1,-2)).tril() @ inp\n",
    "print(y.shape)\n",
    "\n",
    "y2 = causal_dot_product(U,V,inp)\n",
    "print(torch.sqrt(((y-y2)**2).sum()/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488fed47-2a3c-4b93-b133-bd1c3719168f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 512, 16]) torch.Size([56, 512, 16]) torch.Size([56, 16, 16])\n",
      "tensor(0.0031)\n",
      "tensor(0.0049)\n",
      "tensor(0.0049)\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "Uc = U.cuda()\n",
    "Vc = V.cuda()\n",
    "inpc = inp.cuda()\n",
    "\n",
    "Uc = rearrange(Uc, 'b h n r -> (b h) n r').contiguous()\n",
    "Vc = rearrange(Vc, 'b h n r -> (b h) n r').contiguous()\n",
    "inpc = rearrange(inpc, 'b h n 1 -> (b h) n 1').contiguous()\n",
    "\n",
    "# requires double to be accurate!\n",
    "yt = rearrange((Uc.double() @ Vc.transpose(-1,-2).double()).tril() @ inpc.double(), '(b h) n 1 -> b h n 1', b=b).float()\n",
    "\n",
    "y3 = rearrange(causal_product.causal_product_naive_cumsum(Uc, Vc, inpc), '(b h) n 1 -> b h n 1', b=b) \n",
    "\n",
    "\n",
    "Sc = torch.eye(r, device='cuda')[None].expand((b*h, -1, -1))\n",
    "print(Uc.shape, Vc.shape, Sc.shape)\n",
    "y4 = rearrange(causal_product.causal_product_trio(Uc, Sc, Vc, inpc[...,0]), '(b h) n -> b h n 1', b=b)\n",
    "\n",
    "print(torch.sqrt(((y-yt.cpu())**2).sum()))\n",
    "print(torch.sqrt(((y-y3.cpu())**2).sum()))\n",
    "print(torch.sqrt(((y-y4.cpu())**2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db835e02-a5bd-46ea-8952-66f4ad078dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, device='cuda:0')\n",
      "torch.Size([56, 512, 16]) torch.Size([56, 1])\n",
      "tensor(0.0053)\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "from laxtnn.utils.toep_mat import ToepMat\n",
    "\n",
    "Uc = U.cuda()\n",
    "inpc = inp.cuda()\n",
    "\n",
    "Uc = rearrange(Uc, 'b h n r -> (b h) n r').contiguous()\n",
    "inpc = rearrange(inpc, 'b h n 1 -> (b h) n 1').contiguous()\n",
    "\n",
    "yU = (U @ U.transpose(-1,-2)).tril() @ inp\n",
    "\n",
    "ac = torch.ones((1, 1), device='cuda').expand((b*h, -1))\n",
    "Sc = ToepMat(ac, r)\n",
    "print(torch.sqrt(((Sc@Uc.transpose(-1,-2)-Uc.transpose(-1,-2))**2).sum()))\n",
    "\n",
    "print(Uc.shape, ac.shape)\n",
    "y5 = rearrange(causal_product.causal_product_trio_toep(Uc, ac, inpc[...,0]), '(b h) n -> b h n 1', b=b)\n",
    "\n",
    "print(torch.sqrt(((yU-y5.cpu())**2).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876806a-364a-4aa2-808a-ed419596f669",
   "metadata": {},
   "source": [
    "## time forward only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36427e3e-7bef-475c-ad0c-27ab7e3af682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1 ms ± 310 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.cuda.synchronize()\n",
    "((Uc.double() @ Sc.double() @ Vc.transpose(-1,-2).double()).tril() @ inpc.double()).float()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab72b54-e196-4cdd-8f64-a7cd145e72d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 µs ± 220 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.cuda.synchronize()\n",
    "causal_dot_product(Uc[None],(Vc@Sc.transpose(-1,-2))[None],inpc[None])[0]\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b8bf1f-52c5-43c6-9577-5b8209439784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 µs ± 83.1 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.cuda.synchronize()\n",
    "causal_product.causal_product_trio(Uc, Sc, Vc, inpc[...,0])\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a441543-f944-4d35-8735-b2cd74a395c6",
   "metadata": {},
   "source": [
    "## backward too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f562ce3-1e22-4214-9dd1-8160137e18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uc = U.cuda()\n",
    "Vc = V.cuda()\n",
    "Sc = torch.eye(r, device='cuda')[None, None].expand((b, h, -1, -1))\n",
    "inpc = inp.cuda()\n",
    "z = torch.zeros_like(inpc)\n",
    "Uc.requires_grad = False\n",
    "Sc.requires_grad = True\n",
    "Vc.requires_grad = False\n",
    "inpc.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66629fc9-bce0-4929-9770-e8f93103a8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755 µs ± 210 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.cuda.synchronize()\n",
    "y = causal_dot_product(Uc, Vc @ Sc.transpose(-1,-2), inpc)\n",
    "loss = torch.nn.functional.mse_loss(y, z)\n",
    "loss.backward()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a446d4e-e5c8-4564-8393-8e161f2e7b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750 µs ± 290 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.cuda.synchronize()\n",
    "y = causal_dot_product(Uc @ Sc, Vc, inpc)\n",
    "loss = torch.nn.functional.mse_loss(y, z)\n",
    "loss.backward()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a58c5c0-bfde-42fd-8bfa-0c5ef39eba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uc = U.cuda()\n",
    "Vc = V.cuda()\n",
    "Sc = torch.eye(r, device='cuda')[None].expand((b*h, -1, -1))\n",
    "inpc = inp.cuda()\n",
    "Uc = rearrange(Uc, 'b h n r -> (b h) n r').contiguous()\n",
    "Vc = rearrange(Vc, 'b h n r -> (b h) n r').contiguous()\n",
    "inpc = rearrange(inpc, 'b h n 1-> (b h) n').contiguous()\n",
    "z = torch.zeros_like(inpc)\n",
    "Uc.requires_grad = False\n",
    "Sc.requires_grad = True\n",
    "Vc.requires_grad = False\n",
    "inpc.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46ada80a-4839-44e2-a74a-c566b6becc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 µs ± 1.03 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.cuda.synchronize()\n",
    "y = causal_product.causal_product_trio(Uc, Sc, Vc, inpc)\n",
    "loss = torch.nn.functional.mse_loss(y, z)\n",
    "loss.backward()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2d597-44ce-4a66-8575-f940ec9267fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### making sure the gradient values are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae61d5d-f60e-4e03-b7ef-77c9bc3fade4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1766, device='cuda:0')\n",
      "tensor(0.8112, device='cuda:0')\n",
      "tensor(1.1724, device='cuda:0')\n",
      "tensor(0.8827, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "S = torch.rand((1, h, r, r)).expand((b, -1, -1, -1))\n",
    "\n",
    "Uc = U.cuda()\n",
    "Vc = V.cuda()\n",
    "#Sc = torch.eye(r, device='cuda')[None, None].expand((b, h, -1, -1))\n",
    "Sc = S.cuda()\n",
    "inpc = inp.cuda()\n",
    "z = torch.zeros_like(inpc)\n",
    "Uc.requires_grad = False\n",
    "Sc.requires_grad = True\n",
    "Vc.requires_grad = False\n",
    "inpc.requires_grad = True\n",
    "y = causal_dot_product(Uc @ Sc, Vc, inpc)\n",
    "loss = torch.nn.functional.mse_loss(y, z)\n",
    "loss.backward()\n",
    "\n",
    "dX = inpc.grad.detach()\n",
    "dS = Sc.grad.detach()\n",
    "\n",
    "\n",
    "Uc = U.cuda()\n",
    "Vc = V.cuda()\n",
    "#Sc = torch.eye(r, device='cuda')[None, None].expand((b, h, -1, -1))\n",
    "Sc = S.cuda()\n",
    "inpc = inp.cuda()\n",
    "z = torch.zeros_like(inpc)\n",
    "Uc.requires_grad = False\n",
    "Sc.requires_grad = True\n",
    "Vc.requires_grad = False\n",
    "inpc.requires_grad = True\n",
    "y = causal_dot_product(Uc, Vc @ Sc.transpose(-1, -2), inpc)\n",
    "loss = torch.nn.functional.mse_loss(y, z)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "dX1 = inpc.grad.detach()\n",
    "dS1 = Sc.grad.detach()\n",
    "\n",
    "\n",
    "\n",
    "Uc = U.cuda()\n",
    "Vc = V.cuda()\n",
    "#Sc = torch.eye(r, device='cuda')[None].expand((b*h, -1, -1))\n",
    "Sc = rearrange(S.cuda(), 'b h r s -> (b h) r s').contiguous()\n",
    "inpc = inp.cuda()\n",
    "Uc = rearrange(Uc, 'b h n r -> (b h) n r').contiguous()\n",
    "Vc = rearrange(Vc, 'b h n r -> (b h) n r').contiguous()\n",
    "inpc = rearrange(inpc, 'b h n 1-> (b h) n').contiguous()\n",
    "z = torch.zeros_like(inpc)\n",
    "Uc.requires_grad = False\n",
    "Sc.requires_grad = True\n",
    "Vc.requires_grad = False\n",
    "inpc.requires_grad = True\n",
    "y = causal_product.causal_product_trio(Uc, Sc, Vc, inpc)\n",
    "loss = torch.nn.functional.mse_loss(y, z)\n",
    "loss.backward()\n",
    "\n",
    "dX2 = inpc.grad.detach()\n",
    "dS2 = Sc.grad.detach()\n",
    "\n",
    "print(torch.sqrt(((dX1-dX)**2).sum()))\n",
    "print(torch.sqrt(((dS1-dS)**2).sum()))\n",
    "\n",
    "print(torch.sqrt(((rearrange(dX1, 'b h n 1 -> (b h) n')-dX2)**2).sum()))\n",
    "print(torch.sqrt(((rearrange(dS1, 'b h n r -> (b h) n r')-dS2)**2).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744ea8b-9c78-4f48-9ff4-3f43d3ad1d1f",
   "metadata": {},
   "source": [
    "#### following is with `triton==2.0.0.post1`. Also fails with `triton==2.1.0` (segfaults):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05815c8e-ff0e-41d4-827b-a7b6f06f7fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "future feature annotations is not defined (base.py, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ubuntu/miniconda3/envs/tnn/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3343\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-12-57518e9b5b31>\"\u001b[0m, line \u001b[1;32m5\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import triton\n",
      "  File \u001b[1;32m\"/home/ubuntu/miniconda3/envs/tnn/lib/python3.6/site-packages/triton/__init__.py\"\u001b[0m, line \u001b[1;32m12\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from . import impl\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/ubuntu/miniconda3/envs/tnn/lib/python3.6/site-packages/triton/impl/__init__.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .base import builtin, extern, is_builtin\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/ubuntu/miniconda3/envs/tnn/lib/python3.6/site-packages/triton/impl/base.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from __future__ import annotations\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m future feature annotations is not defined\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "TRITON_MIN_BLOCK_SIZE = 16\n",
    "\n",
    "@triton.jit\n",
    "def matvec(X, y):\n",
    "    return tl.sum(X*y[None, :], 1)\n",
    "\n",
    "@triton.jit\n",
    "def test_kernel(\n",
    "    u_ptr,  # (_, n, r)\n",
    "    s_ptr,  # (_, r, r)\n",
    "    v_ptr,  # (_, r, n)  # using transpose to get around a compiler issue\n",
    "    x_ptr,  # (_, n) input vector\n",
    "    L_ptr,  # lower triangular mask of size MIN_BLOCK_SIZE\n",
    "    y_ptr,  # (_, n) output vector\n",
    "    n, r,  # Size of the tensor dimensions\n",
    "    BLOCK_SIZE: tl.constexpr,  # >r\n",
    "    MIN_BLOCK_SIZE: tl.constexpr  # >16, as constrained by triton. Smaller does less redundant computaton though\n",
    "):\n",
    "    # each program works on one batch element\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # Offset by batch size\n",
    "    cur_u_pos = pid * n * r\n",
    "    cur_v_pos = pid * r * n\n",
    "    cur_s_pos = pid * r * r\n",
    "    cur_xy_pos = pid * n\n",
    "\n",
    "    # pointers for one row of U, V, all of S\n",
    "    s_col_ptrs = tl.arange(0, BLOCK_SIZE)\n",
    "    s_row_ptrs = s_col_ptrs  # S is square\n",
    "    s_col_mask = s_col_ptrs < r\n",
    "    s_row_mask = s_col_mask\n",
    "        \n",
    "    # Load all of S as a matrix\n",
    "    s_block_ptrs = cur_s_pos + s_row_ptrs[:, None] * r + s_col_ptrs[None, :]\n",
    "    s_mask = s_row_mask[:, None] & s_col_mask[None, :]\n",
    "    s_offsets = cur_s_pos + s_block_ptrs\n",
    "    S = tl.load(s_ptr + s_offsets, mask=s_mask)\n",
    "    \n",
    "    # Load all of L as a matrix (exactly b x b, so no masking logic needed)\n",
    "    L_row_ptrs = tl.arange(0, MIN_BLOCK_SIZE)\n",
    "    L_col_ptrs = L_row_ptrs  #L is square\n",
    "    L_block_ptrs = L_row_ptrs[:, None] * MIN_BLOCK_SIZE + L_col_ptrs[None, :]\n",
    "    L_offsets = L_block_ptrs\n",
    "    L = tl.load(L_ptr + L_offsets)\n",
    "    \n",
    "    # pointers for blocks of U, V.T, X, Y\n",
    "    u_col_ptrs = s_col_ptrs  # dim same size as S\n",
    "    u_col_mask = s_col_mask\n",
    "    u_row_ptrs = tl.arange(0, MIN_BLOCK_SIZE)\n",
    "    u_block_ptrs = u_row_ptrs[:, None] * r + u_col_ptrs[None, :]\n",
    "    u_mask = (u_row_ptrs[:, None] < n) & (u_col_mask[None, :])\n",
    "    \n",
    "    v_row_ptrs = u_col_ptrs  # U, V same shape, but we're using V.T\n",
    "    v_row_mask = u_col_mask    \n",
    "    v_col_ptrs = u_row_ptrs\n",
    "    v_block_ptrs = v_row_ptrs[:, None] * n + v_col_ptrs[None, :]\n",
    "    v_mask = (v_row_mask[:, None]) & (v_col_ptrs[None, :] < n)\n",
    "    \n",
    "    xy_block_ptrs = tl.arange(0, MIN_BLOCK_SIZE)  # produce corresponding blocks for X, Y\n",
    "    xy_mask = xy_block_ptrs < n\n",
    "    \n",
    "    \n",
    "    # current state [r, ] vector\n",
    "    # FP32 accumulation\n",
    "    Ck = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n",
    "    \n",
    "    # Actual math\n",
    "    loop_len = tl.cdiv(n, MIN_BLOCK_SIZE)\n",
    "    for i in range(0, loop_len):\n",
    "        # Offset for a block of rows in U, V\n",
    "        u_offsets = cur_u_pos + u_block_ptrs\n",
    "        v_offsets = cur_v_pos + v_block_ptrs\n",
    "        xy_offsets = cur_xy_pos + xy_block_ptrs\n",
    "        \n",
    "        # load a block of X as a vector\n",
    "        Xk = tl.load(x_ptr + xy_offsets, mask=xy_mask)  #(b,)\n",
    "        \n",
    "        # Load a block of rows of U and VT as matrices\n",
    "        Uk = tl.load(u_ptr + u_offsets, mask=u_mask)  #(b, r)\n",
    "        VkT = tl.load(v_ptr + v_offsets, mask=v_mask)  #(r, b)\n",
    "        \n",
    "        # Intermediate mat (this can be O(r log r) if we have FFT...)\n",
    "        ## untested because segfault\n",
    "        #VkpT = tl.dot(S, VkT, allow_tf32=False)  #(r, b)\n",
    "        #VkpT = tl.sum(S[:, :, None] * VkT[None, :, :], 1)    #(r, b)\n",
    "        \n",
    "        VkpT = tl.dot(Uk, VkT, allow_tf32=False)\n",
    "        \n",
    "        # Compute output = (U S V^T) X\n",
    "        \n",
    "        #Yk = tl.dot(Uk, VkpT, allow_tf32=False)  #(b, b)\n",
    "        #Yk = matvec(tl.dot(Uk, VkpT, allow_tf32=False), Xk)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        tl.store(y_ptr + xy_offsets, tl.sum(VkpT, 0), mask=xy_mask)\n",
    "        \n",
    "        # prep next iter\n",
    "        if i < loop_len-1:\n",
    "            # Move to next block of rows\n",
    "            cur_u_pos += MIN_BLOCK_SIZE * r\n",
    "            cur_xy_pos += MIN_BLOCK_SIZE\n",
    "            \n",
    "            # Move to next block of columns\n",
    "            cur_v_pos += MIN_BLOCK_SIZE\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Compute output = (L . (U S V^T)) X  [b, b] x [b,] => [b,]\n",
    "        Yk = matvec(Uk, Ck) + matvec(L*tl.dot(Uk, VkpT, allow_tf32=False), Xk)\n",
    "        \n",
    "        # Store the result of this block\n",
    "        tl.store(y_ptr + xy_offsets, Yk, mask=xy_mask)\n",
    "        \n",
    "        # prep next iter\n",
    "        if i < loop_len-1:\n",
    "            # Compute next context [M, b] x [b] => [M]\n",
    "            Ck += matvec(VkpT, Xk)\n",
    "            # Move to next block of rows\n",
    "            cur_uv_pos += MIN_BLOCK_SIZE * r\n",
    "            cur_xy_pos += MIN_BLOCK_SIZE\n",
    "        \"\"\"\n",
    "    \n",
    "def test(u, s, v, x, min_block_size=TRITON_MIN_BLOCK_SIZE):\n",
    "    \"\"\"\n",
    "    Accepts 4 tensors U, S, V, X of shape: [B, n, r], [B, r, r], [B, r, n], and [B, n]\n",
    "    \"\"\"\n",
    "    assert all(x.is_cuda and x.is_contiguous for x in (u,s,v,x))\n",
    "    assert u.size()[0] == v.size()[0]\n",
    "    assert u.size()[1:] == v.size()[:0:-1]\n",
    "    assert u.size()[:-1] == x.size(), (u.size(), x.size())\n",
    "    assert s.size()[-2] == s.size()[-1], s.size()\n",
    "    assert s.size()[-1] == u.size()[-1], (s.size(), u.size())\n",
    "    \n",
    "    # We need to preallocate the output\n",
    "    y = torch.zeros_like(x)\n",
    "    \n",
    "    batch, n, r = u.size()\n",
    "\n",
    "    def grid(meta): return (batch,)\n",
    "    block_size = int(2 ** math.ceil(math.log2(r)))\n",
    "    L = torch.ones((min_block_size, min_block_size), device='cuda').tril()\n",
    "    test_kernel[grid](\n",
    "        u, s, v, x,\n",
    "        L, y, n, r,\n",
    "        BLOCK_SIZE=block_size,\n",
    "        MIN_BLOCK_SIZE=min_block_size\n",
    "    )\n",
    "    return y\n",
    "\n",
    "b = 4\n",
    "n = 64\n",
    "r = 32\n",
    "\n",
    "\n",
    "u = torch.rand((b,n,r), device='cuda')\n",
    "s = torch.rand((b,r,r), device='cuda')\n",
    "v = torch.rand((b,r,n), device='cuda')\n",
    "x = torch.rand((b,n), device='cuda')\n",
    "y_gt = v.sum(-2)\n",
    "y_gt = (s @ v).sum(-2)\n",
    "\n",
    "#y_gt = (u @ v).sum(-2)\n",
    "\n",
    "#y_gt = ((u @ s @ v).tril() @ x[...,None])[...,0]\n",
    "y = test(u, s, v, x, min_block_size=32)\n",
    "#print(s)\n",
    "print(f'error: {torch.sum(torch.abs(y-y_gt)).cpu().numpy()}')\n",
    "\n",
    "print(torch.stack([y,y_gt], dim=2).cpu().numpy())\n",
    "#print(torch.stack([y,y_gt], dim=2)[:16,:16].cpu().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
